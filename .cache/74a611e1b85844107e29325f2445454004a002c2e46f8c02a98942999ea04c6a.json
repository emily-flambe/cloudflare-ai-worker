{
  "url": "https://huggingface.co/docs/inference-providers/en/guides/gpt-oss",
  "markdown": "# How to use OpenAI‚Äôs GPT OSS\n [![Hugging Face's logo](https://huggingface.co/front/assets/huggingface_logo-noborder.svg) Hugging Face](https://huggingface.co/)\n-   [Models](https://huggingface.co/models)\n-   [Datasets](https://huggingface.co/datasets)\n-   [Spaces](https://huggingface.co/spaces)\n-   Community\n-   [Docs](https://huggingface.co/docs)\n-   [Enterprise](https://huggingface.co/enterprise)\n-   [Pricing](https://huggingface.co/pricing)\n-   ---\n-   [Log In](https://huggingface.co/login)\n-   [Sign Up](https://huggingface.co/join)\nInference Providers documentation\nHow to use OpenAI‚Äôs GPT OSS\n# Inference Providers\nüè° View all docsAWS Trainium & InferentiaAccelerateArgillaAutoTrainBitsandbytesChat UIDataset viewerDatasetsDeploying on AWSDiffusersDistilabelEvaluateGradioHubHub Python LibraryHuggingface.jsInference Endpoints (dedicated)Inference ProvidersLeRobotLeaderboardsLightevalMicrosoft AzureOptimumPEFTSafetensorsSentence TransformersTRLTasksText Embeddings InferenceText Generation InferenceTokenizersTransformersTransformers.jssmolagentstimm\nSearch documentation\nmain EN\n[](https://github.com/huggingface/hub-docs)\nGet Started\n[Inference Providers](https://huggingface.co/docs/inference-providers/en/index) [Pricing and Billing](https://huggingface.co/docs/inference-providers/en/pricing) [Hub integration](https://huggingface.co/docs/inference-providers/en/hub-integration) [Register as an Inference Provider](https://huggingface.co/docs/inference-providers/en/register-as-a-provider) [Security](https://huggingface.co/docs/inference-providers/en/security)\nProviders\n[Cerebras](https://huggingface.co/docs/inference-providers/en/providers/cerebras) [Cohere](https://huggingface.co/docs/inference-providers/en/providers/cohere) [Fal AI](https://huggingface.co/docs/inference-providers/en/providers/fal-ai) [Featherless AI](https://huggingface.co/docs/inference-providers/en/providers/featherless-ai) [Fireworks](https://huggingface.co/docs/inference-providers/en/providers/fireworks-ai) [Groq](https://huggingface.co/docs/inference-providers/en/providers/groq) [Hyperbolic](https://huggingface.co/docs/inference-providers/en/providers/hyperbolic) [HF Inference](https://huggingface.co/docs/inference-providers/en/providers/hf-inference) [Nebius](https://huggingface.co/docs/inference-providers/en/providers/nebius) [Novita](https://huggingface.co/docs/inference-providers/en/providers/novita) [Nscale](https://huggingface.co/docs/inference-providers/en/providers/nscale) [Replicate](https://huggingface.co/docs/inference-providers/en/providers/replicate) [SambaNova](https://huggingface.co/docs/inference-providers/en/providers/sambanova) [Together](https://huggingface.co/docs/inference-providers/en/providers/together)\nGuides\n[Your First API Call](https://huggingface.co/docs/inference-providers/en/guides/first-api-call) [Building Your First AI App](https://huggingface.co/docs/inference-providers/en/guides/building-first-app) [Structured Outputs with LLMs](https://huggingface.co/docs/inference-providers/en/guides/structured-output) [Function Calling](https://huggingface.co/docs/inference-providers/en/guides/function-calling) [How to use OpenAI's GPT OSS](https://huggingface.co/docs/inference-providers/en/guides/gpt-oss)\nAPI Reference\n[Index](https://huggingface.co/docs/inference-providers/en/tasks/index) [Hub API](https://huggingface.co/docs/inference-providers/en/hub-api)\nPopular Tasks\n[Chat Completion](https://huggingface.co/docs/inference-providers/en/tasks/chat-completion) [Feature Extraction](https://huggingface.co/docs/inference-providers/en/tasks/feature-extraction) [Text to Image](https://huggingface.co/docs/inference-providers/en/tasks/text-to-image) [Text to Video](https://huggingface.co/docs/inference-providers/en/tasks/text-to-video)\nOther Tasks\n![Hugging Face's logo](https://huggingface.co/front/assets/huggingface_logo-noborder.svg)\nJoin the Hugging Face community\nand get access to the augmented documentation experience\nCollaborate on models, datasets and Spaces\nFaster examples with accelerated inference\nSwitch between documentation themes\n[Sign Up](https://huggingface.co/join)\nto get started\n# [](#how-to-use-openais-gpt-oss)How to use OpenAI‚Äôs GPT OSS\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/inference-providers-guides/gpt-oss-thumbnail-light.png)\nThis guide walks you through using OpenAI‚Äôs latest GPT OSS models with Hugging Face Inference Providers. GPT OSS is an open-weights family built for strong reasoning, agentic workflows and versatile developer use cases, and it comes in two sizes: a one with 120B parameters ([gpt-oss-120b](https://hf.co/openai/gpt-oss-120b)), and a smaller one with 20B parameters ([gpt-oss-20b](https://hf.co/openai/gpt-oss-120b)).\nBoth models are supported on Inference Providers and can be accessed through either the OpenAI-compatible [Chat Completions API](https://platform.openai.com/docs/api-reference/chat/completions), or the more advanced [Responses API](https://platform.openai.com/docs/api-reference/responses).\n## [](#quickstart)Quickstart\n1.  You‚Äôll need your Hugging Face token. Get one from your [settings page](https://huggingface.co/settings/tokens/new?ownUserPermissions=inference.serverless.write&tokenType=fineGrained). Then, set it as an environment variable.\nCopied\nexport HF\\_TOKEN=\"your\\_token\\_here\"\nüí° Pro tip: The free tier gives you monthly inference credits to start building and experimenting. Upgrade to [Hugging Face PRO](https://huggingface.co/pro) for even more flexibility, $2 in monthly credits plus pay‚Äëas‚Äëyou‚Äëgo access to all providers!\n2.  Install the official OpenAI SDK.\npython\njavascript\nCopied\npip install openai\n## [](#chat-completion)Chat Completion\nGetting started with GPT OSS models on Inference Providers is simple and straightforward. The OpenAI-compatible Chat Completions API supports features like tool calling, structured outputs, streaming, and reasoning effort controls.\nHere‚Äôs a basic example using [gpt-oss-120b](https://hf.co/openai/gpt-oss-120b) through the fast Cerebras provider:\npython\njavascript\nCopied\nimport os\nfrom openai import OpenAI\nclient = OpenAI(\n    base\\_url=\"https://router.huggingface.co/v1\",\n    api\\_key=os.getenv(\"HF\\_TOKEN\"),\n)\nresponse = client.chat.completions.create(\n    model=\"openai/gpt-oss-120b:cerebras\",\n    messages=\\[{\"role\": \"user\", \"content\": \"Tell me a fun fact about the Eiffel Tower.\"}\\],\n)\nprint(response.choices\\[0\\].message.content)\nYou can also give the model access to tools. Below, we define a `get_current_weather` function and let the model decide whether to call it:\npython\njavascript\nCopied\nimport os\nfrom openai import OpenAI\nclient = OpenAI(\n    base\\_url=\"https://router.huggingface.co/v1\",\n    api\\_key=os.getenv(\"HF\\_TOKEN\"),\n)\ntools = \\[\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get\\_current\\_weather\",\n            \"description\": \"Get the current weather in a given location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n                    },\n                    \"unit\": {\"type\": \"string\", \"enum\": \\[\"celsius\", \"fahrenheit\"\\]},\n                },\n                \"required\": \\[\"location\"\\],\n            },\n        },\n    }\n\\]\nresponse = client.chat.completions.create(\n    model=\"openai/gpt-oss-120b:cerebras\",\n    messages=\\[{\"role\": \"user\", \"content\": \"What is the weather in Paris in Celsius?\"}\\],\n    tools=tools,\n    tool\\_choice=\"auto\",\n)\n\\# The response will contain the tool\\_calls object if the model decides to use the tool\nprint(response.choices\\[0\\].message)\nFor structured tasks like data extraction, you can force the model to return a valid JSON object using the `response_format` parameter. We use the Fireworks AI provider.\npython\njavascript\nCopied\nimport json\nimport os\nfrom openai import OpenAI\nclient = OpenAI(\n    base\\_url=\"https://router.huggingface.co/v1\",\n    api\\_key=os.getenv(\"HF\\_TOKEN\"),\n)\n\\# Force the model to output a JSON object\nresponse = client.chat.completions.create(\n    model=\"openai/gpt-oss-120b:fireworks-ai\",\n    messages=\\[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful assistant designed to output JSON.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract the name, city, and profession from the following sentence: 'Am√©lie is a chef who lives in Paris.'\",\n        },\n    \\],\n    response\\_format={\n        \"type\": \"json\\_schema\",\n        \"json\\_schema\": {\n            \"name\": \"person\",\n            \"schema\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"name\": {\"type\": \"string\"},\n                    \"city\": {\"type\": \"string\"},\n                    \"profession\": {\"type\": \"string\"},\n                },\n                \"required\": \\[\"name\", \"city\", \"profession\"\\],\n            },\n        },\n    },\n)\n\\# The output is a valid JSON string that can be easily parsed\noutput\\_json\\_string = response.choices\\[0\\].message.content\nparsed\\_output = json.loads(output\\_json\\_string)\nprint(parsed\\_output)\nWith just a few lines of code, you can start using GPT OSS models with Hugging Face Inference Providers, fully OpenAI API-compatible, easy to integrate, and ready out of the box!\n## [](#responses-api)Responses API\nInference Providers implements the **OpenAI-compatible Responses API**, the most advanced interface for chat-based models. It supports streaming, structured outputs, tool calling, reasoning effort controls (low, medium, hard), and Remote MCP calls to delegate tasks to external services.\nKey Advantages:\n-   Agent-Oriented Design: The API is specifically built to simplify workflows for agentic tasks. It has a native framework for integrating complex tool use, such as Remote MCP calls.\n-   Stateful, Event-Driven Architecture: Features a stateful, event-driven architecture. Instead of resending the entire text on every update, it streams semantic events that describe only the precise change (the ‚Äúdelta‚Äù). This eliminates the need for manual state tracking.\n-   Simplified Development for Complex Logic: The event-driven model makes it easier to build reliable applications with multi-step logic. Your code simply listens for specific events, leading to cleaner and more robust integrations.\nThe implementation is based on the open-source [huggingface/responses.js](https://github.com/huggingface/responses.js) project.\n### [](#stream-responses)Stream responses\nUnlike traditional text streaming, the Responses API uses a system of semantic events for streaming. This means the stream is not just raw text, but a series of structured event objects. Each event has a type, so you can listen for the specific events you care about, such as content being added (`output_text.delta`) or the message being completed (`completed`). The example below shows how to iterate through these events and print the content as it arrives.\npython\njavascript\nCopied\nimport os\nfrom openai import OpenAI\nclient = OpenAI(\n    base\\_url=\"https://router.huggingface.co/v1\",\n    api\\_key=os.getenv(\"HF\\_TOKEN\"),\n)\n\\# Set stream=True to receive a stream of semantic events\nstream = client.responses.create(\n    model=\"openai/gpt-oss-120b:fireworks-ai\",\n    input\\=\"Tell me a short story about a robot who discovers music.\",\n    stream=True,\n)\n\\# Iterate over the events in the stream\nfor event in stream:\n    print(event)\n### [](#tool-calling)Tool Calling\nYou can extend the model with tools to access external data. The example below defines a get\\_current\\_weather function that the model can choose to call.\npython\njavascript\nCopied\nfrom openai import OpenAI\nimport os\nclient = OpenAI(\n    base\\_url=\"https://router.huggingface.co/v1\",\n    api\\_key=os.getenv(\"HF\\_TOKEN\"),\n)\ntools = \\[\n    {\n        \"type\": \"function\",\n        \"name\": \"get\\_current\\_weather\",\n        \"description\": \"Get the current weather in a given location\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"location\": {\n                    \"type\": \"string\",\n                    \"description\": \"The city and state, e.g. San Francisco, CA\",\n                },\n                \"unit\": {\"type\": \"string\", \"enum\": \\[\"celsius\", \"fahrenheit\"\\]},\n            },\n            \"required\": \\[\"location\", \"unit\"\\],\n        },\n    }\n\\]\nresponse = client.responses.create(\n    model=\"openai/gpt-oss-120b:fireworks-ai\",\n    tools=tools,\n    input\\=\"What is the weather like in Boston today?\",\n    tool\\_choice=\"auto\",\n)\nprint(response)\n### [](#remote-mcp-calls)Remote MCP Calls\nThe API‚Äôs most advanced feature is Remote MCP calls, which allow the model to delegate tasks to external services. Calling a remote MCP server with the Responses API is straightforward. For example, here‚Äôs how you can use the DeepWiki MCP server to ask questions about nearly any public GitHub repository.\npython\njavascript\nCopied\nimport os\nfrom openai import OpenAI\nclient = OpenAI(\n    base\\_url=\"https://router.huggingface.co/v1\",\n    api\\_key=os.getenv(\"HF\\_TOKEN\"),\n)\nresponse = client.responses.create(\n    model=\"openai/gpt-oss-120b:fireworks-ai\",\n    input\\=\"What transport protocols are supported in the 2025-03-26 version of the MCP spec?\",\n    tools=\\[\n        {\n            \"type\": \"mcp\",\n            \"server\\_label\": \"deepwiki\",\n            \"server\\_url\": \"https://mcp.deepwiki.com/mcp\",\n            \"require\\_approval\": \"never\",\n        },\n    \\],\n)\nprint(response)\n### [](#reasoning-effort)Reasoning Effort\nYou can also control the model‚Äôs ‚Äúthinking‚Äù time with the `reasoning` parameter. The following example nudges the model to spend a medium amount of effort on the answer.\npython\njavascript\nCopied\nfrom openai import OpenAI\nimport os\nclient = OpenAI(\n    base\\_url=\"https://router.huggingface.co/v1\",\n    api\\_key=os.getenv(\"HF\\_TOKEN\"),\n)\nresponse = client.responses.create(\n    model=\"openai/gpt-oss-120b:fireworks-ai\",\n    instructions=\"You are a helpful assistant.\",\n    input\\=\"Say hello to the world.\",\n    reasoning={\n        \"effort\": \"low\",\n    },\n)\nfor index, item in enumerate(response.output):\n    print(f\"Output #{index}: {item.type}\", item.content)\nThat‚Äôs it! With the Responses API on Inference Providers, you get fine-grained control over powerful open-weight models like GPT OSS, including streaming, tool calling, and remote MCP, making it ideal for building reliable, agent-driven applications.\n[< \\> Update on GitHub](https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/guides/gpt-oss.md)\n[‚ÜêFunction Calling](https://huggingface.co/docs/inference-providers/en/guides/function-calling) [Index‚Üí](https://huggingface.co/docs/inference-providers/en/tasks/index)\n[How to use OpenAI‚Äôs GPT OSS](#how-to-use-openais-gpt-oss) [Quickstart](#quickstart) [Chat Completion](#chat-completion) [Responses API](#responses-api) [Stream responses](#stream-responses) [Tool Calling](#tool-calling) [Remote MCP Calls](#remote-mcp-calls) [Reasoning Effort](#reasoning-effort)",
  "timestamp": 1754619365927,
  "title": "How to use OpenAI‚Äôs GPT OSS"
}