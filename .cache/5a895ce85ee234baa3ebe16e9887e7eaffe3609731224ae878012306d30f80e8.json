{
  "url": "https://claude.ai/public/artifacts/ac5886ca-e54a-4465-bbfd-a781a6d06cfd",
  "markdown": "# GPT-OSS Cloudflare Workers AI Implementation Specification | Claude | Claude\nLoading...\n[media](/isolated-segment.html)\n# GPT-OSS Cloudflare Workers AI Implementation Specification\n## Executive Summary\nTransition existing Cloudflare Workers AI implementation to utilize OpenAI's new gpt-oss models (gpt-oss-120b and gpt-oss-20b) available as Day 0 launch on Cloudflare's Workers AI platform.\n## Model Information\n### Available Models\n1.  **Production Model**: `@cf/openai/gpt-oss-120b`\n    -   117B total parameters, 5.1B active parameters per token\n    -   Best for: Complex reasoning, production workloads, high-accuracy requirements\n    -   Memory requirement: 80GB (runs on single H100 GPU)\n    -   Context window: 128k tokens\n2.  **Edge Model**: `@cf/openai/gpt-oss-20b`\n    -   21B total parameters, 3.6B active parameters per token\n    -   Best for: Lower latency, edge deployments, cost-sensitive applications\n    -   Memory requirement: 16GB\n    -   Context window: 128k tokens\n## Implementation Requirements\n### Prerequisites\n-   Cloudflare account with Workers AI enabled\n-   Valid Cloudflare API token with Workers AI permissions\n-   Cloudflare Account ID\n-   Existing Worker project or new Worker setup\n### Environment Variables Required\njavascript\n```javascript\nCLOUDFLARE_ACCOUNT_ID    // Your Cloudflare account ID\nCLOUDFLARE_API_KEY       // API token with Workers AI access\n```\n## Implementation Approaches\n### Approach 1: Workers AI Binding (Recommended for New Projects)\n#### Setup - wrangler.toml Configuration\ntoml\n```toml\nname = \"gpt-oss-worker\"\nmain = \"src/index.js\"\ncompatibility_date = \"2024-08-05\"\n[ai]\nbinding = \"AI\"\n```\n#### Implementation Code\njavascript\n```javascript\nexport default {\n  async fetch(request, env, ctx) {\n    try {\n      const { prompt, model = \"gpt-oss-120b\", reasoning = \"medium\" } = await request.json();\n      // Model selection logic\n      const modelPath = model === \"gpt-oss-20b\"\n        ? \"@cf/openai/gpt-oss-20b\"\n        : \"@cf/openai/gpt-oss-120b\";\n      // Call the model using Workers AI binding\n      const response = await env.AI.run(modelPath, {\n        messages: [\n          { role: \"system\", content: \"You are a helpful assistant.\" },\n          { role: \"user\", content: prompt }\n        ],\n        reasoning: { effort: reasoning } // \"low\", \"medium\", or \"high\"\n      });\n      return new Response(JSON.stringify({\n        success: true,\n        model: modelPath,\n        response: response\n      }), {\n        headers: { 'Content-Type': 'application/json' }\n      });\n    } catch (error) {\n      return new Response(JSON.stringify({\n        success: false,\n        error: error.message\n      }), {\n        status: 500,\n        headers: { 'Content-Type': 'application/json' }\n      });\n    }\n  }\n};\n```\n### Approach 2: Responses API (REST Endpoint)\n#### Implementation Code\njavascript\n```javascript\nexport default {\n  async fetch(request, env, ctx) {\n    try {\n      const { prompt, model = \"gpt-oss-120b\", reasoning = \"medium\" } = await request.json();\n      const modelPath = model === \"gpt-oss-20b\"\n        ? \"@cf/openai/gpt-oss-20b\"\n        : \"@cf/openai/gpt-oss-120b\";\n      const response = await fetch(\n        `https://api.cloudflare.com/client/v4/accounts/${env.CLOUDFLARE_ACCOUNT_ID}/ai/v1/responses`,\n        {\n          method: 'POST',\n          headers: {\n            'Content-Type': 'application/json',\n            'Authorization': `Bearer ${env.CLOUDFLARE_API_KEY}`\n          },\n          body: JSON.stringify({\n            model: modelPath,\n            reasoning: { effort: reasoning },\n            input: [\n              {\n                role: \"user\",\n                content: prompt\n              }\n            ]\n          })\n        }\n      );\n      const result = await response.json();\n      return new Response(JSON.stringify({\n        success: true,\n        model: modelPath,\n        response: result\n      }), {\n        headers: { 'Content-Type': 'application/json' }\n      });\n    } catch (error) {\n      return new Response(JSON.stringify({\n        success: false,\n        error: error.message\n      }), {\n        status: 500,\n        headers: { 'Content-Type': 'application/json' }\n      });\n    }\n  }\n};\n```\n### Approach 3: OpenAI SDK Compatible Implementation\n#### Installation\nbash\n```bash\nnpm install openai\n```\n#### Implementation Code\njavascript\n```javascript\nimport OpenAI from \"openai\";\nexport default {\n  async fetch(request, env, ctx) {\n    try {\n      const { prompt, model = \"gpt-oss-120b\", reasoning = \"medium\" } = await request.json();\n      const openai = new OpenAI({\n        apiKey: env.CLOUDFLARE_API_KEY,\n        baseURL: `https://api.cloudflare.com/client/v4/accounts/${env.CLOUDFLARE_ACCOUNT_ID}/ai/v1`,\n      });\n      const modelPath = model === \"gpt-oss-20b\"\n        ? \"@cf/openai/gpt-oss-20b\"\n        : \"@cf/openai/gpt-oss-120b\";\n      const response = await openai.responses.create({\n        model: modelPath,\n        input: prompt,\n        reasoning: { effort: reasoning }\n      });\n      return new Response(JSON.stringify({\n        success: true,\n        model: modelPath,\n        response: response\n      }), {\n        headers: { 'Content-Type': 'application/json' }\n      });\n    } catch (error) {\n      return new Response(JSON.stringify({\n        success: false,\n        error: error.message\n      }), {\n        status: 500,\n        headers: { 'Content-Type': 'application/json' }\n      });\n    }\n  }\n};\n```\n## AI Gateway Integration (Optional but Recommended)\n### Benefits\n-   Request logging and analytics\n-   Caching for repeated queries\n-   Rate limiting\n-   Cost tracking\n### Setup Steps\n1.  Create AI Gateway in Cloudflare Dashboard\n2.  Note the gateway ID\n3.  Update API endpoints\n### Modified Endpoint Pattern\nReplace:\n```\nhttps://api.cloudflare.com/client/v4/accounts/{account_id}/ai/run/{model_id}\n```\nWith:\n```\nhttps://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/workers-ai/{model_id}\n```\n### Implementation with AI Gateway\njavascript\n```javascript\nconst GATEWAY_ID = \"your-gateway-id\";\nconst response = await fetch(\n  `https://gateway.ai.cloudflare.com/v1/${env.CLOUDFLARE_ACCOUNT_ID}/${GATEWAY_ID}/workers-ai/${modelPath}`,\n  {\n    method: 'POST',\n    headers: {\n      'Authorization': `Bearer ${env.CLOUDFLARE_API_KEY}`,\n      'Content-Type': 'application/json'\n    },\n    body: JSON.stringify({\n      messages: [{ role: \"user\", content: prompt }],\n      reasoning: { effort: reasoning }\n    })\n  }\n);\n```\n## Advanced Features\n### 1\\. Reasoning Effort Control\njavascript\n```javascript\n// Adjust computational effort for reasoning\nconst reasoningLevels = {\n  fast: \"low\",      // Fastest response, basic reasoning\n  balanced: \"medium\", // Default, balanced performance\n  deep: \"high\"      // Maximum reasoning capability\n};\n```\n### 2\\. Multi-turn Conversations\njavascript\n```javascript\nconst messages = [\n  { role: \"system\", content: \"You are a helpful coding assistant.\" },\n  { role: \"user\", content: \"Write a Python function to sort a list\" },\n  { role: \"assistant\", content: \"Here's a Python function...\" },\n  { role: \"user\", content: \"Now make it more efficient\" }\n];\nconst response = await env.AI.run(\"@cf/openai/gpt-oss-120b\", {\n  messages: messages,\n  reasoning: { effort: \"medium\" }\n});\n```\n### 3\\. Tool Calling / Function Support\njavascript\n```javascript\nconst response = await env.AI.run(\"@cf/openai/gpt-oss-120b\", {\n  messages: [{ role: \"user\", content: prompt }],\n  tools: [\n    {\n      type: \"function\",\n      function: {\n        name: \"get_weather\",\n        description: \"Get current weather\",\n        parameters: {\n          type: \"object\",\n          properties: {\n            location: { type: \"string\" }\n          }\n        }\n      }\n    }\n  ],\n  tool_choice: \"auto\"\n});\n```\n## Migration Checklist\n### Phase 1: Preparation\n-   [ ]  Verify Cloudflare account has Workers AI access\n-   [ ]  Obtain and securely store API credentials\n-   [ ]  Document current model being used\n-   [ ]  Identify all Workers using AI models\n-   [ ]  Review current prompt templates\n### Phase 2: Development\n-   [ ]  Create development Worker for testing\n-   [ ]  Implement chosen approach (Binding/REST/SDK)\n-   [ ]  Add error handling and retry logic\n-   [ ]  Implement request/response logging\n-   [ ]  Add model selection logic (120b vs 20b)\n### Phase 3: Testing\n-   [ ]  Test with sample prompts from production\n-   [ ]  Compare outputs with current model\n-   [ ]  Benchmark response times\n-   [ ]  Test error scenarios\n-   [ ]  Validate reasoning levels\n### Phase 4: Deployment\n-   [ ]  Deploy to staging environment\n-   [ ]  Configure AI Gateway (if using)\n-   [ ]  Update environment variables\n-   [ ]  Deploy to production with feature flag\n-   [ ]  Monitor performance metrics\n### Phase 5: Optimization\n-   [ ]  Analyze usage patterns\n-   [ ]  Optimize model selection (120b vs 20b)\n-   [ ]  Implement caching where appropriate\n-   [ ]  Fine-tune reasoning levels\n-   [ ]  Document best practices\n## Performance Considerations\n### Model Selection Guidelines\nUse **gpt-oss-120b** when:\n-   Complex reasoning required\n-   High accuracy is critical\n-   Production workloads\n-   Cost is less important than quality\nUse **gpt-oss-20b** when:\n-   Lower latency required\n-   Edge deployment\n-   Simple to moderate complexity tasks\n-   Cost optimization is important\n### Optimization Tips\n1.  Implement response caching for repeated queries\n2.  Use appropriate reasoning levels (don't use \"high\" for simple tasks)\n3.  Batch requests when possible\n4.  Implement proper error handling with exponential backoff\n5.  Monitor token usage and costs\n## Error Handling\n### Common Error Scenarios\njavascript\n```javascript\nconst errorHandler = {\n  429: \"Rate limit exceeded - implement backoff\",\n  500: \"Internal server error - retry with exponential backoff\",\n  400: \"Invalid request - check model path and parameters\",\n  401: \"Authentication failed - verify API key\",\n  403: \"Forbidden - check account permissions\"\n};\n```\n### Retry Implementation\njavascript\n```javascript\nasync function callWithRetry(fn, maxRetries = 3) {\n  for (let i = 0; i < maxRetries; i++) {\n    try {\n      return await fn();\n    } catch (error) {\n      if (i === maxRetries - 1) throw error;\n      await new Promise(resolve => setTimeout(resolve, Math.pow(2, i) * 1000));\n    }\n  }\n}\n```\n## Monitoring and Logging\n### Key Metrics to Track\n-   Response latency\n-   Token usage per request\n-   Error rates by type\n-   Model selection distribution (120b vs 20b)\n-   Reasoning level usage\n-   Cache hit rates (if using AI Gateway)\n### Sample Logging Structure\njavascript\n```javascript\nconst logEntry = {\n  timestamp: new Date().toISOString(),\n  model: modelPath,\n  reasoning: reasoning,\n  prompt_length: prompt.length,\n  response_time: responseTime,\n  status: \"success\" | \"error\",\n  error_message: error?.message\n};\n```\n## Security Best Practices\n1.  **Never expose API keys in client-side code**\n2.  **Use Cloudflare Secrets for sensitive data**\n3.  **Implement rate limiting per user/IP**\n4.  **Validate and sanitize all input prompts**\n5.  **Log requests for audit purposes**\n6.  **Use CORS headers appropriately**\n## Cost Optimization\n### Strategies\n1.  Use gpt-oss-20b for non-critical queries\n2.  Implement aggressive caching via AI Gateway\n3.  Set appropriate max\\_tokens limits\n4.  Monitor and alert on unusual usage patterns\n5.  Use \"low\" reasoning effort for simple queries\n## Support and Resources\n### Documentation Links\n-   Cloudflare Workers AI: [https://developers.cloudflare.com/workers-ai/](https://developers.cloudflare.com/workers-ai/)\n-   Model Documentation: [https://developers.cloudflare.com/workers-ai/models/gpt-oss-120b/](https://developers.cloudflare.com/workers-ai/models/gpt-oss-120b/)\n-   AI Gateway Setup: [https://developers.cloudflare.com/ai-gateway/](https://developers.cloudflare.com/ai-gateway/)\n-   OpenAI Compatibility: [https://developers.cloudflare.com/workers-ai/configuration/open-ai-compatibility/](https://developers.cloudflare.com/workers-ai/configuration/open-ai-compatibility/)\n### Getting Help\n-   Cloudflare Discord Community\n-   Cloudflare Support (for paid plans)\n-   Stack Overflow tag: `cloudflare-workers`\n-   GitHub Issues on Cloudflare repos\n## Acceptance Criteria\nThe migration is considered complete when:\n1.  All Workers successfully use gpt-oss models\n2.  Response quality meets or exceeds previous model\n3.  Latency is within acceptable bounds (<2s p95)\n4.  Error rate is below 1%\n5.  Monitoring and alerting are configured\n6.  Documentation is updated\n7.  Team is trained on new features\n---\n_Last Updated: August 2025_ _Version: 1.0_\nMade with[](https://claude.ai)\nArtifacts are user-generated and may contain unverified or potentially unsafe content.\nCustomize",
  "timestamp": 1754619436521,
  "title": "GPT-OSS Cloudflare Workers AI Implementation Specification | Claude | Claude"
}